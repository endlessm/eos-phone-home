#!/usr/bin/python

'''Evaluate apache logs from send-census.

This maintains a local SQLite database for the historic data and the current
state, and can produce text and chart outputs.
'''

__author__ = 'Matt Zimmerman <matt.zimmerman@canonical.com>, Martin Pitt <martin.pitt@canonical.com>'
__copyright__ = '(C) 2010 Canonical Ltd.'
__license__ = 'GPL v2 or later'

import random
import re
import sys
import time
import datetime
import os
import os.path
import urlparse
import operator
import optparse
import sqlite3 as dbapi2
import subprocess
import tempfile
import gzip
import GeoIP
import csv
import fileinput

# highest plausible counter; we assume that each computer sends at most 3 pings
# a day (which is a lot for cron.daily), every day since the start of the
# project
max_counter = (datetime.date.today() - datetime.date(2010, 7, 25)).days * 3
if max_counter < 100:
    max_counter = 100

class Counter:
    def __init__(self):
        self.counters = []

    def add(self, generation):
        need_counters = generation + 1 - len(self.counters)
        self.counters += [0] * need_counters

        if generation > 0 and self.counters[generation-1] > 0:
            self.counters[generation-1] -= 1
        self.counters[generation] += 1

    def dump(self):
        print "Generations: %d" % len(self.counters)
        print "Histogram:"
        for i in range(len(self.counters)):
            if self.counters[i] > 0:
                print "%d: %d" % (i, self.counters[i])

    def count(self):
        return reduce(operator.__add__, self.counters)

class Simulator:
    def __init__(self):
        self.clients = []
        self.counter = Counter()

    def iterate(self, n=1):
        for i in range(n):
            for client in self.clients:
                if client.test():
                    self.counter.add(client.increment())

    def test(self):
        class TestClient:
            def __init__(self):
                self.generation = 0

            def test(self): return True

            def increment(self):
                gen = self.generation
                self.generation += 1
                return gen

        class RandomFailureClient(TestClient):
            def __init__(self, error_rate):
                """0 < error_rate <= 100"""

                TestClient.__init__(self)
                self.error_rate = error_rate

            def test(self):
                if random.randint(0,100) > self.error_rate:
                    return True
                return False

        # add 50 clients which randomly fail from 0-50% of the time
        self.clients.extend([RandomFailureClient(random.randint(0,50)) for i in range(50)])

        # run for 100 iterations
        self.iterate(100)

        # add 20 clients which always succeed
        self.clients.extend(TestClient() for i in range(20))

        # run for 25 iterations
        self.iterate(25)

        # add 30 clients which were incrementing but not phoning home until now
        stale_clients = [TestClient() for i in range(30)]
        for iteration in range(30):
            for client in stale_clients:
                client.increment()
        self.clients.extend(stale_clients)

        # run for 25 iterations
        self.iterate(25)

        # add 30 clients which were randomly incrementing but not phoning home until now
        stale_clients = [RandomFailureClient(random.randint(0,50)) for i in range(30)]
        for iteration in range(30):
            for client in stale_clients:
                if client.test():
                    client.increment()
        self.clients.extend(stale_clients)

        # run for 25 iterations
        self.iterate(25)

        self.counter.dump()
        assert self.counter.count() == len(self.clients)

class State:
    '''State of the entire system'''

    def __init__(self, dbpath):
        '''Initialize state from a database
        
        If the db does not exist yet, it will be created.
        '''
        init = not os.path.exists(dbpath)
        self.db = dbapi2.connect(dbpath)

        if init:
            cur = self.db.cursor()
            cur.execute('''CREATE TABLE db_version (
                version INT NOT NULL)''')

            cur.execute('''CREATE TABLE last_update (
                timestamp DOUBLE NOT NULL)''')

            cur.execute('''CREATE TABLE counters (
                channel_id PRIMARY KEY,
                counters TEXT)''')

            cur.execute('''CREATE TABLE history (
                channel_id INT NOT NULL,
                date CHAR(12),
                day INT NOT NULL,
                count INT NOT NULL,
                PRIMARY KEY (channel_id, date))''')

            cur.execute('''CREATE TABLE channel (
                channel_id PRIMARY KEY,
                dcd CHAR(100),
                vendor CHAR(100),
                product CHAR(100),
                country CHAR(3))''')

            cur.execute('INSERT INTO db_version VALUES (0)')
            cur.execute("INSERT INTO last_update VALUES (0.0)")

            self.db.commit()

    def update_from_log(self, path, map_dcd, after=None):
        '''Update status from Apache log.
        
        This will only include items which are newer than the last timestamp.

        map_dcd is a (regexp, name) list for mapping all DCDs matching regexp
        to a meta-channel "name".
        '''
        census_re = re.compile('([\S][\S])[\s\S]+\[(.+?) [-+]\d{4}\] "GET .*/submit\?([^\s"]+).*?"')

        cur = self.db.cursor()
        cur.execute('SELECT timestamp FROM last_update')
        last_update = cur.fetchone()[0]

        channel_counters = self._counters_from_db()
        stats = self._current_stats()
        channels = self._current_channels()
        geoip = GeoIP.new(GeoIP.GEOIP_MEMORY_CACHE)

        if path.endswith('.gz'):
            f = gzip.open(path)
        else:
            f = open(path)

        for line in f:
            m = census_re.match(line)
            if not m:
                continue
            #print m.group(1)
            #print m.group(2)
            #print m.group(3)
            timestamp = time.mktime(time.strptime(m.group(2), '%d/%b/%Y:%H:%M:%S'))
            if after and timestamp < after:
                #print 'ignoring too early date', line
                continue

            if timestamp < last_update:
                #print 'ignoring previously seen line', line
                continue
            last_update = max(timestamp, last_update)

            channel_ids = []
            args = urlparse.parse_qs(m.group(3))
            try:
                dcd = args['dcd'][0]
                assert len(dcd) > 0

                # map DCD
                for (r, name) in map_dcd:
                    if r.match(dcd):
                        dcd = name
                        break

                count = int(args['count'][0])
            except (TypeError, ValueError, KeyError):
                print 'ERROR! Invalid census query:', m.group(3)
                continue

            if count < 0 or count > max_counter:
                #print 'ERROR! Ignoring invalid counter:', line 
                continue

            product = ""
            if 'product' in args:
                product = args['product'][0]

            vendor = ""
            if 'vendor' in args:
                vendor = args['vendor'][0]
            
            # try to resolve country
            country = m.group(1)
            if not country:
                country = ""
                
            # If channel exists, find it. If not, create a new one w/ new channel ID
            channel = (dcd, vendor, product, country)
            channel_id = self._get_channel_id(channels, channel)
            channels[channel] = channel_id
            channel_ids.append(channel_id)

            #print 'timestamp: %f (%s), channels: %s, count: %i' % (timestamp, m.group(2), str(channels), count)

            date = time.strftime('%Y-%m-%d', time.localtime(timestamp))

            for c in channel_ids:
                channel_counters.setdefault(c, Counter()).add(count)
                st = stats.setdefault(c, {}).setdefault(date, [0,0])
                st[0] += 1
                st[1] = channel_counters[c].count()

        cur.execute('DELETE FROM last_update')
        cur.execute('INSERT INTO last_update VALUES (?)', (last_update+.1,))
        self._counters_to_db(channel_counters)
        self._set_current_stats(stats)
        self._set_current_channels(channels)
        self.db.commit()

    def dump(self):
        #print '====== COUNTERS ========'
        #for channel, counter in self._counters_from_db().iteritems():
        #    print '---- %s ---' % channel
        #    print 'machines:', counter.count()
        #    print 'hist:', counter.counters

        cur = self.db.cursor()
        cur.execute('SELECT DISTINCT channel_id FROM history')
        channels = [x[0] for x in cur.fetchall()]
        print '====== HISTORY ========'
        for ch in channels:
            print '---- channel id: %s -----' % ch

            cur.execute('SELECT date, day, count, dcd, vendor, product, country FROM channel NATURAL JOIN history WHERE channel_id = ? ORDER BY date', 
                    (ch,))

            for (date, day, count, dcd, vendor, product, country) in cur:
                print '%s %s %s %s' % (dcd, vendor, product, country)
                print '%s: %4i updates sent, %4i machines total' % (date, day, count)
                
    def csv_generic(self, sql=None, filename=None, header=None, start=None, end=None):
        if sql is None:
            print "Error: You must supply an SQL query"
            sys.exit(1)
        if filename is None:
            print "Error: You must supply an output filename"
            sys.exit(1)
        if start is None or end is None:
            print "Error: You must supply a --start_date and --end_date!"
            sys.exit(1)
        else:
            cur = self.db.cursor()
            cur.execute(sql, (start, end))
            filename = filename % (start, end)
            with open(filename, 'wb') as f:
                writer = csv.writer(f)
                writer.writerow(['Check-ins for %s through %s' % (start, end)])
                if header is not None:
                    writer.writerow(header)
                writer.writerows(cur)
            f.close()

    def csv(self, start, end):
        sql = 'SELECT date, day, count, channel_id, dcd, vendor, product, country  FROM channel NATURAL JOIN history WHERE date BETWEEN ? AND ?'
        filename = 'census-data-%s-%s.csv'
        header = ['date', 'check-ins', 'number-of-machines', 'channel_id', 'image', 'vendor', 'product', 'country']
        self.csv_generic(sql, filename, header, start, end)

    def csv_geo(self, start=None, end=None):
        sql = 'SELECT sum(day), max(count), country FROM channel NATURAL JOIN history WHERE date BETWEEN ? AND ? GROUP BY country'
        filename = 'census-by-geo-%s-%s.csv'
        header = ['check-ins', 'number-of-machines', 'GEO']
        self.csv_generic(sql, filename, header, start, end)

    def csv_image(self, start, end):
        sql = 'SELECT sum(day), max(count), dcd FROM channel NATURAL JOIN history WHERE date BETWEEN ? AND ? GROUP BY dcd'
        filename = 'census-by-image-%s-%s.csv'
        header = ['check-ins', 'number-of-machines', 'OEM Image Name']
        self.csv_generic(sql, filename, header, start, end)

    def csv_image_geo(self, start, end):
        sql = 'SELECT sum(day), max(count), dcd, country FROM channel NATURAL JOIN history WHERE date BETWEEN ? AND ? GROUP BY dcd, country'
        filename = 'census-by-image-and-geo-%s-%s.csv'
        header = ['check-ins', 'number-of-machines', 'OEM Image Name', 'GEO']
        self.csv_generic(sql, filename, header, start, end)

    def csv_vendor(self, start, end):
        sql = 'SELECT sum(day), max(count), vendor FROM channel NATURAL JOIN history WHERE date between ? and ? GROUP BY vendor'
        filename = 'census-by-vendor-%s-%s.csv'
        header = ['check-ins', 'number-of-machines', 'Vendor Name']
        self.csv_generic(sql, filename, header, start, end)

    def csv_vendor_geo(self, start, end):
        sql = 'SELECT sum(day), max(count), vendor, country FROM channel NATURAL JOIN history WHERE date BETWEEN ? AND ? GROUP BY vendor, country'
        filename = 'census-by-vendor-and-geo-%s-%s.csv'
        header = ['check-ins', 'number-of-machines', 'Vendor Name', 'GEO']
        self.csv_generic(sql, filename, header, start, end)
    
    def csv_model(self, start, end):
        sql = 'SELECT sum(day), max(count), vendor, product FROM channel NATURAL JOIN history WHERE date BETWEEN ? AND ? GROUP BY product'
        filename = 'census-by-model-%s-%s.csv'
        header = ['check-ins', 'number-of-machines', 'Vendor Name', 'Model Name']
        self.csv_generic(sql, filename, header, start, end)

    def csv_model_geo(self, start, end):
        sql = 'SELECT sum(day), max(count), vendor, product, country FROM channel NATURAL JOIN history WHERE date BETWEEN ? AND ? GROUP BY product, country'
        filename = 'census-by-model-and-geo-%s-%s.csv'
        header = ['check-ins', 'number-of-machines', 'Vendor Name', 'Model Name', 'GEO']
        self.csv_generic(sql, filename, header, start, end)
          
    def plot(self, directory, daily_pings=False):
        '''Generate gnuplot charts.

        This will create <channel>.png files in given output directory.
        If daily_pings is True, draw bars with the number of received pings on
        every day; this is not a very meaningful number, and thus is disabled
        by default.
        '''
        cur = self.db.cursor()
        cur.execute('SELECT DISTINCT channel_id FROM history')
        channels = [x[0] for x in cur.fetchall()]

        for ch in channels:
            gnuplot = subprocess.Popen(['gnuplot'], stdin=subprocess.PIPE)
            print >> gnuplot.stdin, '''set xdata time
set timefmt "%Y-%m-%d"
set terminal png
'''
            f = tempfile.NamedTemporaryFile()
            cur.execute('SELECT date, day, count, dcd, vendor, product, country FROM history NATURAL JOIN channel WHERE channel_id = ? ORDER BY date', 
                    (ch,))
            for (date, day, count, dcd, vendor, product, country) in cur:
                print >> f, '%s\t%i\t%i\t%s\t%s\t%s\t%s' % (date, day, count, dcd, vendor, product, country)

            f.flush()
            
            plotname = '%s-%s-%s-%s' % (dcd, vendor, product, country)
            print >> gnuplot.stdin, 'set out "%s.png"\nset title "%s"' % (
                    os.path.join(directory, plotname), plotname)
            print >> gnuplot.stdin, 'set yrange [0:]\nset xtics rotate'

            if daily_pings:
                cmd = 'plot "%s" using 1:3 title "#machines" with lines lw 3 lt 1 , \
                  "%s" using 1:2 title "#updates on that day" with impulses lw 2 lt 3' % (f.name, f.name)
            else:
                cmd = 'plot "%s" using 1:3 title "" with line lw 3 lt 1' % f.name
            print >> gnuplot.stdin, cmd

            print >> gnuplot.stdin, 'exit'
            assert gnuplot.wait() == 0, 'gnuplot failed with %i' % gnuplot.returncode

    def _counters_from_db(self):
        '''Return a channel->Counter map from DB.'''

        counters = {}
        cur = self.db.cursor()
        cur.execute('SELECT * FROM counters')
        for channel, counter_str in cur:
            counters[channel] = Counter()
            counters[channel].counters = eval(counter_str, {}, {})

        return counters

    def _counters_to_db(self, map):
        '''Write channel->Counter map to DB.'''

        cur = self.db.cursor()
        cur.execute('DELETE FROM counters')
        for (channel, counters) in map.iteritems():
            cur.execute('INSERT INTO counters VALUES (?, ?)', (channel,
                    repr(counters.counters)))

    def _current_stats(self):
        '''Get most recent per-day/counter stats.

        Return channel->date->[day, count] map.
        '''
        cur = self.db.cursor()
        cur.execute('SELECT timestamp FROM last_update')
        last_update = time.strftime('%Y-%m-%d',
                time.localtime(float(cur.fetchone()[0])))

        map = {}
        cur.execute('SELECT channel_id, day, count FROM history WHERE date = ?',
                (last_update,))
        for (channel_id, day, count) in cur:
            map.setdefault(channel_id, {})[last_update] = [day, count]

        return map

    def _set_current_stats(self, stats):
        '''Set most recent per-day/counter stats.'''

        cur = self.db.cursor()
        for channel_id, per_date in stats.iteritems():
            for date, (day, count) in per_date.iteritems():
                cur.execute('INSERT OR REPLACE INTO history VALUES (?, ?, ?, ?)',
                        (channel_id, date, day, count))

    def _current_channels(self):
        '''Get map of channel descriptions mapped to channel ID.

        Return map of [dcd, vendor, product, country], channel_id'''
        channels = {}
        cur = self.db.cursor()
        cur.execute('SELECT channel_id, dcd, vendor, product, country FROM channel')
        for (channel_id, dcd, vendor, product, country) in cur:
            channels.setdefault((dcd,vendor,product,country), channel_id)
            
        return channels

    def _set_current_channels(self, channels):
        '''Set channel descriptions.'''

        cur = self.db.cursor()
        for (dcd, vendor, product, country), channel_id in channels.iteritems():
            cur.execute('INSERT OR REPLACE INTO channel VALUES (?, ?, ?, ?, ?)',
                        (channel_id, dcd, vendor, product, country))

    def _next_channel_id(self, channels):
        '''Return a unique channel_id.'''
        if len(channels.values()) > 0:
            return max(channels.values()) + 1
        else:
            return 1

    def _get_channel_id(self, channels, channel):
        '''Return the channel ID of a channel. Assigns a new channel ID if necessary'''
        if channel not in channels:
            return self._next_channel_id(channels)
        else:
            return channels[channel]
                

def parse_args():
    '''Parse command line args.

    Return (options, args) tuple.
    '''

    parser = optparse.OptionParser(usage='\n  %prog [options]\n  %prog [options] -l logfile [logfile ..]')
    parser.add_option('-t', '--test', action='store_true',
            help='Run simulator for testing the algorithm')
    parser.add_option('-d', '--database', metavar='PATH',
            help='Path to database')
    parser.add_option('--text', action='store_true',
            help='Dump current history to stdout.')
    parser.add_option('-l', '--log', action='store_true',
            help='Update data with Apache log file(s).')
    parser.add_option('--after', metavar='YYYY-MM-DD',
            help='Only consider updates after given date')
    parser.add_option('--map-dcd', action='append', metavar='name:REGEXP',
            default=[], help='Map all distribution channel descriptors matching REGEXP to a meta-channel "name"')
    parser.add_option('-g', '--gnuplot', metavar='DIR', action='store',
            help='Generate graphs to given output directory')
    parser.add_option('--daily', action='store_true', default=False, 
            help='Show number of received daily updates on plots')
    parser.add_option('--start_date', metavar='YYYY-MM-DD',
            help='Used with csv file generation to capture date ranges in YYYY-MM-DD format')
    parser.add_option('--end_date', metavar='YYYY-MM-DD',
            help='Used with csv file generation to capture date ranges in YYYY-MM-DD format')
    parser.add_option('--csv', action='store_true',
            help='Dump entire database history into a csv file for a given period. Requires --start_date and --end_date')
    parser.add_option('--csv_geo', action='store_true',
            help='Dump check-ins by country into a csv file for a given period. Requires --start_date and --end_date')
    parser.add_option('--csv_image', action='store_true',
            help='Dump check-ins by OEM image name into a csv file for a given period. Requires --start_date and --end_date')
    parser.add_option('--csv_image_geo', action='store_true',
            help='Dump check-ins by OEM image and geography into a csv file for a given period.  Requires --start_date and --end_date') 
    parser.add_option('--csv_vendor', action='store_true',
            help='Dump check-ins by manufacturer name into a csv file for a given period.  Requires --start_date and --end_date')
    parser.add_option('--csv_vendor_geo', action='store_true',
            help='Dump check-ins by manufacturer and geography into a csv file for a given period.  Requires --start_date and --end_date')
    parser.add_option('--csv_model', action='store_true',
            help='Dump check-ins by model name into a csv file for a given period.  Requires --start_date and --end_date')
    parser.add_option('--csv_model_geo', action='store_true',
            help='Dump check-ins by model name and geography into a csv file for a given period.  Requires --start_date and --end_date')
    
   
    (opts, args) = parser.parse_args()

    if not opts.test:
        if not opts.database:
            parser.error('ERROR: You need to specify a database with --database.  See --help')

    if opts.log:
        if len(args) == 0:
            parser.error('ERROR: You need to specify at least one log file. See --help')

    if opts.after:
        try:
            opts.after = time.mktime(time.strptime(opts.after, '%Y-%m-%d'))
        except ValueError:
            parser.error('ERROR: Invalid --after date')

    # turn --map-channel into a (regexp, name) list
    map_dcd = []
    for arg in opts.map_dcd:
        try:
            name, regexp = arg.split(':', 1)
            assert len(name) > 0
        except (AssertionError, ValueError):
            parser.error('ERROR: --map-channel argument must be "name:REGEXP"')
        try:
            regexp = re.compile(regexp)
        except Exception, e:
            parser.error('ERROR: Invalid regular expression in --map-channel: %s' %
                    str(e))
        map_dcd.append((regexp, name))
    opts.map_dcd = map_dcd

    return (opts, args)

def main():
    (opts, args) = parse_args()

    if opts.test:
        Simulator().test()
        sys.exit(0)

    state = State(opts.database)

    if opts.log:
        fnames = []
        for f in args:
            fnames.append(f)

        lines = list(fileinput.input(fnames))
        t_fmt = '%d/%b/%Y:%H:%M:%S'
        t_pat = re.compile('([\S][\S])[\s\S]+\[(.+?) [-+]\d{4}\] "GET .*/submit\?([^\s"]+).*?"')
        outFile = open('combined.log', 'w')
        for l in sorted(lines, key=lambda l: time.strptime(t_pat.search(l).group(2), t_fmt)):
           # outFile = open('combined.log', 'a')
            outFile.write(l,)
        outFile.close()
        state.update_from_log('combined.log', opts.map_dcd, opts.after)


    if opts.text:
        state.dump()

    if opts.gnuplot:
        state.plot(opts.gnuplot, opts.daily)

    if opts.csv:
        state.csv(opts.start_date,opts.end_date)

    if opts.csv_geo:
        state.csv_geo(opts.start_date,opts.end_date)

    if opts.csv_image:
        state.csv_image(opts.start_date,opts.end_date)

    if opts.csv_image_geo:
        state.csv_image_geo(opts.start_date,opts.end_date)
  
    if opts.csv_vendor:
        state.csv_vendor(opts.start_date, opts.end_date)
    
    if opts.csv_vendor_geo:
        state.csv_vendor_geo(opts.start_date, opts.end_date)

    if opts.csv_model:
        state.csv_model(opts.start_date, opts.end_date)

    if opts.csv_model_geo:
        state.csv_model_geo(opts.start_date, opts.end_date)

if __name__ == '__main__':
    main()
